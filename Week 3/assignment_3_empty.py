# -*- coding: utf-8 -*-
"""Assignment 3 - Empty.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kVvnXQFsdiWtOsQAfKyYhaXqEBksPdm0

# Turing Machine and Deep Learning 2023

## Assignment Unupervised ML

This notebook is meant for you to review and reflect on the content of Lecture 3, which was mainly about unsupervised learning problems. As with last week, this notebook should not be too much quantitative work (lines of code) but keep in mind that running this notebook may take a longer time than you may be used to for python programs (*training good models take time!*)

### Handing in your Assignment

Git is an invaluable resource to researchers and developers, and thus for this course, all course material will be (additionally) shared on GitHub. Though there is a tiny bit of a learning curve, this is worth the effort. To hand in your assignment (applicable to all weeks):

1. Create a folder called "Week 2" and copy this notebook and any other files or data that may be needed.
2. Finish the notebook and commit and push regularly. Your final commit before the deadline will be graded.


### Grading

Each one of the (sub-)questions below will be graded either 0 (insufficient), 1 (sufficient) or 2 (good). If $N$ is the number of sub-questions, and $p_i$ is your score for sub-question $i$, your total grade $G$ for this assignment is:
$$G=\frac{1}{2 N}\sum_{i=0}^{N}p_i$$
"""

# load common libraries
import numpy as np                 # maths
import matplotlib.pyplot as plt    # plotting
import pandas as pd                # data manipulation
from tqdm import tqdm              # loading bar
from time import perf_counter      # timer

"""# Q1 Loading and preprocessing data

In this repository, you should find a file called `tmdb_5000_movies.csv` which is information from a subset of movies on The Movie Database.

### Q.1.1
Use pandas to read in the csv file (refer to [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) if you're unfamiliar).
"""

df = pd.read_csv("tmdb_5000_movies TMDL 2024.csv")
df

"""View information about the dataset including datatypes and null values"""

df.info()

"""**Q.1.1.2** What columns have null values above? How would you be able to estimate the missing values?

Homepage, overview, release_date, runtime, tagline. To estimate the missing values you can text similar algorithms to find similar overviews, use the median to fill in missing values or the mode.

### Q1.2
For this notebook, we're only interested in a few columns, specifically title, overview and genres. Set df to only contain those columns.
"""

columns_to_keep = ["title", "overview", "genres"]
df = df[columns_to_keep]
df.info() # print out df info to verify

"""### Q1.3
We see that some movies do not have an overview. Drop them from the table and reset the index (set `drop=True`).
"""

df = df.dropna(axis=0, how='any')
df = df.reset_index()
df.info()

"""# Q2 One-hot encodings
The following code processes the genres (which you see above are list of dictionaries) and encodes them into one-hot labels.

Remember, since we cannot do maths on text strings, we must change these words to numbers. Here, we create one-hot encodings for the genres. Assume we have three genres $[G_1, G_2, G_3]$. If a movie $M_1$ is tagged with genre $G_1$, the encoding is $[1,0,0]$. If another movie $M_2$ is tagged with $G2$, the encoding becomes $[0,1,0]$. Other encodings are possible as well, but one-hot encoding quite common and useful for many other tasks. In this case, if a movie is tagged with more than one genre, we'll just take a random one and encode that into a one-hot label.

### Q2.1 Processing genres

Finish the following code that processes genres into one-hot encodings.
"""

np.random.seed(0)

def process_genre_string(gs):
    """ Processes genres into a single item from a list"""
    gs = eval(gs)
    gs = [x['name'] for x in gs]
    genre = "Unknown"
    if gs:
        genre = np.random.choice(gs) # choose random entry
    return genre

# TODO: Fill in
def encode_labels(label, unique_labels):
    """ Encodes text labels into a one-hot encoded list
        with possibly more than one one-hot per list.
        :param label: the label you want to one-hot encode
        :unique_labels: the vocabulary
    """
    out = np.zeros(len(unique_labels))
    out[unique_labels.index(label)] = 1
    return out

# save genres
processed_genres = []
for index, row in df.iterrows():
    processed_genres.append(process_genre_string(df.iat[index, 3]))

# sort and remove duplicates to get vocabulary
unique_genres = sorted(list(set(processed_genres)))

# add to dataframe as new column
df["proc_genres"] = processed_genres

# one-hot encode genres
encoded_labels = []
for index, row in tqdm(df.iterrows(), total=len(df)):
    encoded_labels.append(encode_labels(processed_genres[index], unique_genres))

df["one_hot_genres"] = encoded_labels

"""Verify that one-hot encodings are indeed as expected"""

df["one_hot_genres"][0]

"""### Q2.2 Processing Overviews
Next, we process the overviews. First, we remove all punctuation for the sake of simplicity and change each overview to use only lowercase. Then we need to see how long the overviews are in terms of numbers of words. We can do this with a histogram.

**Q2.2.1** Finish the following code
"""

import re
def remove_punctuation(text):
    """ Only retains letters, numbers, underscores and whitespace """
    pattern = r'[^\w\s]'
    return re.sub(pattern, '', text)

processed_overviews = []

# loop over df, remove punctuation, make words lowercase and add it to a new column
for index, row in df.iterrows():
    processed_overviews.append(remove_punctuation(df.iat[index, 2]).lower())
df["proc_overview"] = processed_overviews

# get the word lengths of each overview and store it in a list
overview_lens = []
for string in processed_overviews:
  overview_lens.append(len(string))

# for the sake of simplicity, add these values as a column to the df
df["overview_len"] = overview_lens
plt.hist(df['overview_len'])
plt.show()

"""**Q2.2.2** What is the shortest overview? What is the longest? In the next step, we need to decide on a standard length of all overviews -- this means dropping overviews less than some value, and truncating longer ones. What length would you choose to minimize the number of movies dropped and maximize the information (words) stored in the overviews?

For this we can do a code for max and min. For choosing a standard length you must retain enough information from the overvies and min. the number of dropped movies.

### Q2.2.3
Let's choose overviews of length 15 words. What this means is we need to discard movies that have overviews less than 15 (there are other ways of dealing with it, but this should be fine for now) and truncate the higher ones to the first 15 words.
"""

token_len = 15 # number of words
# only select rows where overview len is more than or equal to token_len
df = df[df['overview_len'] >= 15]

# split each proc_overview into a list of words, select the first token_len words,
# and add the list of words back into df["proc_overview"]
df["proc_overview"] = df["proc_overview"].apply(lambda x: ' '.join(x.split()[:token_len]))

# print to verify
df['proc_overview']

"""### Q2.2.3 Finding the vocabulary length

In order to one-hot encode words, we need to find how many words there are in total, just like in the case of genres. Get all the words, remove duplicates, and sort. Find and print the length of your vocabulary.
"""

# hint: store all words for all movies in a set, change it to a list and sort
joint_overviews = []

for index, row in df.iterrows():
    for word in row['proc_overview']:
      joint_overviews.append(word)
vocab_overv = sorted(list(set(joint_overviews)))

vocab_len = len(vocab_overv)
print(vocab_len)
print(vocab_overv)

"""### Q2.2.4 Encoding the labels

In the case of genres, we one-hot encoded the outputs by taking a single random genre. However, an alternative method is to add up the one-hot encodings to form some kind of histogram. For example, if we have an overview "a brown dog", and our vocab is \[a brown, big, cabbage, dog, goat, cow, turkey\], the one-hot vector would be \[1,1,0,0,1,0,0,0\]. If our overview is "a big brown dog", the one-hot vector would be \[1,1,1,0,1,0,0,0\]. If our overview is "a big big brown dog", the one-hot vector would be \[1,1,2,0,1,0,0,0\]. You can use the `encode_labels` function that you defined earlier.
"""

# this code is just a hint, if you want you can do it as you please
# as long as the output remains the same
encoded_labels = []
for index, row in tqdm(df.iterrows(), total=len(df)):
    sentence_encode = [] # set of encodings for this overview
    for word in row["proc_overview"]:
        sentence_encode.append(encode_labels(word, vocab_overv)) # get encoding for this word
    sentence_encode = np.sum(sentence_encode, axis=0) # sum over axis=1
    encoded_labels.append(sentence_encode)

df["one_hot_overview"] = encoded_labels
df['one_hot_overview'] # print to verify

"""**Q2.2.5** Each vector is a vector of floating point (64-bit) numbers. Assuming each float takes up exactly 16-bytes, how many bytes does this take to store (theoretically)?

967296000 bytes

# Q3 Principal Component Analysis

**Q3.1** Using the overview encoded into one-hot encodings, perform PCA and plot this into a 2-D image as a scatter plot.
"""

# import PCA decomposition class from sklearn
from sklearn.decomposition import PCA

# we did exactly this in the lecture notebook
pca = PCA(n_components = 2)
overview_pca = pca.fit(np.array(encoded_labels))
overview_pca = pca.transform(np.array(encoded_labels))

plt.figure(figsize=(8, 6))
plt.scatter(overview_pca[:, 0], overview_pca[:, 1], cmap='tab10', alpha=0.6, s=1)
plt.colorbar()
plt.show()

"""**Q3.2** Do you see any interpretable structure in the above plot ("interpretable" $\rightarrow$ the patterns are explainable).

You need the actual plot or data to see the structure.

**Q3.3** Quantify how much variance information a 2D PCA projection loses for this dataset.

You can calculate the explained variance ratio for the 2 principal components.

**Q3.4** Plot a line graph where the y-axis is cumulative explained variance ratio and the x-axis is the number of PCA components.
"""

# Set up PCA object with the desired number of components
pca2 = PCA(n_components=2)
pca_decomp2 = pca2.fit(np.array(encoded_labels))
powers = pca2.explained_variance_ratio_

plt.plot(powers)
plt.plot([np.sum(powers[:i]) for i in range(len(powers)-1)])
plt.show()

"""**Q3.5** How many principal components do you need in order to explain 80% of the total variance in the data?

Note: don't just estimate it by eye, write some code to compute it for you.
"""

# your code answer here
total_variance = 0
for i  in range(len(powers)):
  total_variance = total_variance + powers[i]
  if total_variance >= 0.8:
    print(str(i + 1))
    break

"""*Your text answer here*

**Q3.6** Using the number of dimensions you found in Q3.5, fit and transform your overview encodings using PCA and add it to a new column called `overview_pca` in the dataframe.
"""

n_dims = 2
pca_final = PCA(n_components= 2)
labels_pca = pca_final.fit(np.array(encoded_labels))
labels_pca = pca_final.transform(np.array(encoded_labels))
labels_pca = labels_pca.tolist()
df["overview_pca"] = labels_pca

"""# Q4 K-Means Clustering

**Q4.1** Cluster the movies based on the features that were extracted via PCA in the last step. Set $K=20$. Add the predicted cluster into the dataframe as a new column called `cluster_kmeans`. Print out the elements of cluster number 0 from the dataframe.
"""

# import KMeans class
from sklearn.cluster import KMeans

# Perform K-means clustering
kmeans = KMeans(n_clusters=20)
kmeans.fit(labels_pca)

y_preds = kmeans.predict(labels_pca)
df["cluster_kmeans"] = y_preds # set predictions

# print out elements of cluster 0
cluster_index = 0
cluster_indices = np.where(kmeans.labels_ == cluster_index)[0]
for index in cluster_indices:
  print(df['title'][index])

"""**Q4.2** Does this clustering seem alright to you (based on your movie watching history)? Are there movies that go well together and movies that don't?

*Your text answer here*

**Q4.3** Now, we'll figure out whether using the elbow method is right for this dataset. Plot a loss (using `kmeans.inertia_`) versus cluster size plot. Is there an elbow that you see clearly? What cluster size would you choose?

*Your text answer here*
"""



wcss = []
k_values = range(1, 21)
for k in k_values:
  kmeans = KMeans(n_clusters=k)
  kmeans.fit(labels_pca)
  wcss.append(kmeans.inertia_)

plt.plot(k_values, wcss, marker='o')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.title('Elbow Method')
plt.show()
 # your code answer here

"""# Q5 Gaussian Mixture Models

**Q5.1** As with the K-means above, cluster the movies based on the features that were extracted via PCA in a previous step. Set $K=20$. Add the predicted cluster into the dataframe as a new column called `cluster_gmm`. Print out the elements of a single cluster of your choice from the dataframe.
"""

from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=20)
gmm.fit(labels_pca)
df['cluster_gmm'] = gmm.predict(labels_pca)

for i in range(len(labels_pca)):
  if gmm.predict([labels_pca[i]]) == 1:
    print(df['title'][i])
# Your code answer here

"""**Q4.2** Does this clustering seem alright to you (based on your movie watching history)? Are there movies that go well together and movies that don't? How does this compare to K-Means Clustering?

You can see the clustering works because iron man and iron man 3 are together.

**Q4.3** Let's check out the size of the clusters. Create a double bar plot (**as you did in the last assignment**) showing the different sizes of the clusters.

*Hint: you may want to consider `df.value_counts()` if you're unfamiliar.*
"""

# Create an array for the x-axis positions
clusternums = np.arange(20)
clustersizes = np.bincount(gmm.predict(labels_pca))

fig, ax = plt.subplots()
# Plotting the bars
ax.bar(clusternums, clustersizes)
# Add labels, title, and legend
ax.set_xlabel('Cluster')
ax.set_ylabel('Size')
ax.set_title('Cluster Sizes')
ax.set_xticks(clusternums)
ax.set_xticklabels(clusternums)
plt.show()

"""**Q4.4** Do you see a significant difference in the sizes of the clusters? Which is more uniform? Any thought on why?

there is a significant difference between the clusters.

# Q6: A very simple recommender system

One useful thing that clustering is often used for (though at a much greater complexity) is in **recommender systems**. These are systems with users and items (movies, files, documents, etc.), where new items are shown to the user based on what they've previously interacted with, and possibly also on the behaviour of other users.

**Q6.1** Assume your dentist has just watched the movie Avatar and asks you for a recommendation. Lucky for you, you just finished this assignment. Using the cluster indices of the movie Avatar for both the K-means and GMM methods, print out suggestions for new movies.
"""

prev_watch = "Avatar"

indexmovie = df['title'].tolist().index(prev_watch)
gmm_cluster = df['cluster_gmm'][indexmovie]
kmeans_cluster = df['cluster_kmeans'][indexmovie]

for i in range(len(df['title'].tolist())):
  if df['cluster_gmm'][i] == gmm_cluster:
    print(df['title'][i])

for i in range(len(df['title'].tolist())):
  if df['cluster_kmeans'][i] == kmeans_cluster:
    print(df['title'][i])

"""**Q6.2** Are any of the two recommender systems any good? Would you use them if your real dentist asks for a movie suggestion?

I wouldn't use any in real life, though they give similar suggestions.

**Q6.3** How would you try making the recommender systems better?

More explanatory variables.

**Q6.4** Say your dentist likes the movies you suggested and has watched a few more since you met him last. How would you incorporate this fact (recommendation based on multiple movies) into your suggestions?

More than one cluster.

**Q6.5** An alternative method for recommendations is to use your encoded movie-feature vectors (in this case your overview+PCA vector) in order to find out what movie may be most similar to the current one. In the case of K-Means and GMMs, "similarity" referred to Euclidean distance. However, in this exercise, we will use *cosine similarity*, which is another very common similarity measure, and is related to the angle between two vectors. It is defined as:

$$sim(v_1, v_2)=\frac{v_1\cdot v_2}{||v_1||\cdot||v_2||}$$

Where $v_1$ and $v_2$ are vectors and the operator $||\cdot||$ is the norm of the vector. The function ranges from $[-1,1]$ (where 1 means that the vectors point in the same direction).

Define an appropriate function `cosine_sim`. Add a column called `cos_sim_to_avatar` in the df and print out the head of the df, sorted by the similarity. What movies would you recommend now? Is this better than the clustering methods?
"""

def cosine_sim(v1, v2):
    dot_product = np.dot(v1, v2)
    magnitude_v1 = np.linalg.norm(v1)
    magnitude_v2 = np.linalg.norm(v2)
    return dot_product/(magnitude_v2 * magnitude_v1)

# get overview_pca encoding of previously watched film
prev_watch_enc = df['overview_pca'][indexmovie]

sims = [] # set up place to store similarities
for i, row in df.iterrows():
    sims.append(cosine_sim(prev_watch_enc, row['overview_pca']))

df["cos_sim_to_avatar"] = sims
df = df.sort_values('cos_sim_to_avatar') # sort df
df # print to verify

"""*Your text answer here*

# Q7 Unsupervised Learning as a Tool

At the end of this brief forey into natural language processing, we will end with an interesting task: can a machine predict the genre of a movie based purely on the first 10 words of its overview?

First, the following cell shuffles the dataset and splits it into a training and test set. The random seed and random state ensure that the train and test samples are the same for you all the time, and *probably* your classmates too. You can verify this by printing out the dataframes and checking it for yourself.
"""

np.random.seed(0)
n_train = int(0.8*len(df))
df_shuffle = df.sample(frac=1, random_state=42).reset_index(drop=True)
df_train = df_shuffle[:n_train].reset_index(drop=True)
df_test = df_shuffle[n_train:].reset_index(drop=True)
print(len(df_train), len(df_test))

"""## Train, test, evaluate

Using a supervised learning method of your choice, try predicting the genre of a movie from the overview. The way you preprocess your data is up to you, and you can take inspiration from the above exercises (e.g. PCA on the one-hot encodings as input, one-hot label encodings as output). There are a number of hyperparameters you can choose depending on your selected method (number of words in your overview, number of dimensions, number of clusters, hyperparameters of your supervised model...), thus, make sure to perform hyperparameter optimization in some way (grid-search, fine-tuning, etc). Once you are happy with how your model is performing, **print out the train_score (accuracy$\in[0,1]$), test_score (accuracy$\in[0,1]$), mean training time (in seconds), and mean inference time (in seconds) of your model**.

If you are already excited about neural networks, you may also choose to use that as your supervised method. The easiest way to do it is with sklearn's `MLPClassifier` module. The main hyperparameter you would need to tune is the architecture of your model -- how many hidden layers, and how large is each one. For this task, in order to get best performance, you do not necessarily need an MLP (multi-layer perceptron) but feel free to experiment.

*Hint: the `train_and_eval` function from the last assignment should be of great help here*
"""

# I am not nearly sure how to begin to do this :)) But i really have tried. your code here: feel free to add extra cells

"""Write a paragraph of ~150 words about how you went about selecting and tuning your model, and how you may want to make improvements to your model if you were to continue working on this. Plots are very good but not strictly necessary (i.e. try to add plots if you can).

*Your text answer here*

# BONUS

If you are happy with your score and wish to see how well it is doing with respect to other people (a sort of mini-competition), fill out the following form with the train_score (accuracy$\in[0,1]$), test_score (accuracy$\in[0,1]$), mean training time (in seconds), and mean inference time (in seconds) of your model, as well as what method you use. You can submit as many times before the deadline as you wish. Your final results will be correlated with your submission in order to validate your results (if we cannot validate them, they will be immediately disqualified).

[FORM HERE](https://forms.gle/rXRtXScABH5oDLRWA)
"""